#Import all the necessary libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

from numpy import nan as na
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import ADASYN
from sklearn.preprocessing import StandardScaler as sd
from sklearn.linear_model import LogisticRegression as lg
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.svm import LinearSVC,SVC
from lightgbm import LGBMClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import f1_score,accuracy_score,roc_auc_score,roc_curve
from sklearn.metrics import precision_score,recall_score

import pandas as pd

# URL of the dataset
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00468/online_shoppers_intention.csv"

# Load the dataset directly from the URL
data = pd.read_csv(url)

# Display the first few rows
data.head()

# tail means printing the last 5 records
data.tail(5) 

# check null values
data.isnull().sum()

# concise summary of the DataFrame data
data.info()

# shape which is really helpful to find number of rows and columns
data.shape 

# when we describe our data set we could see our mean,meadian,min,max and all
data.describe() 

# names of the independent features (also known as predictor variables or input variables)
print('Independent Features: ',list(data.columns[:-1]))

import pandas as pd

# 'data' is the initial DataFrame
data.columns

# To get value counts for the 'Revenue' column
data['Revenue'].value_counts()

#concise summary of the DataFrame data, including information about the index dtype and column dtypes
data.info()

# Create a new DataFrame (data_converted) as a copy of the original data
data_converted = data.copy()

# Now, check the information of the new DataFrame
data_converted.info()


#Convert from real numbers to categorical
# DataFrame named 'data'
# want to convert: 'Month', 'OperatingSystems', 'Browser', 'Region', 'TrafficType', 'VisitorType'

# Convert 'Month' to categorical
data_converted['Month'] = data_converted['Month'].astype('category')

# Convert 'OperatingSystems', 'Browser', 'Region', 'TrafficType', 'SpecialDay' to categorical
categorical_columns = ['OperatingSystems', 'Browser', 'Region', 'TrafficType', 'SpecialDay']
data_converted[categorical_columns] = data_converted[categorical_columns].astype('category')

# Convert 'VisitorType' to categorical (assuming it's non-ordinal)
data_converted['VisitorType'] = pd.Categorical(data_converted['VisitorType'])

# Save the new dataset to a CSV file
data_converted.to_csv('data_2.csv', index=False)

data_converted.info()

data_converted.columns

#Analysis and Visualization
#Here below we splited the numerical and catagorical columns

num_cols=[col for col in data_converted.select_dtypes(include=np.number)]
cat_cols=[col for col in data_converted.select_dtypes(exclude=np.number)]

print('Numerical column:',len(num_cols),'catagorcal column:',len(cat_cols))
print('Numerical Column Names:',num_cols)
print('catagorcial Column Names:',cat_cols)

# here below we created the density plot for numerical columns
# Import necessary libraries
import matplotlib.pyplot as plt

# Create density plots for numerical columns
data[num_cols].plot(kind='density', subplots=True, layout=(4, 4), sharex=False, figsize=(25, 12))
#plt.show()

#Print the box plot for numerical col
data_converted[num_cols].plot(kind='box',subplots=True,layout=(4,4),figsize=(25,12))

data_converted.groupby('Month')['Revenue'].value_counts().unstack('Revenue').plot(kind='bar',stacked=True,figsize=(10,5))

data_converted.groupby('Weekend')['Revenue'].value_counts().unstack('Revenue').plot(kind='bar',stacked=True,figsize=(7,7))

data_converted.VisitorType.value_counts().plot.pie(y='VisitorType',figsize=(7,7))

data_converted['VisitorType'].value_counts()

admin_dpt=data_converted[['Administrative_Duration','ProductRelated_Duration','Informational_Duration','VisitorType']]

pd.pivot_table(admin_dpt,values=['Administrative_Duration','ProductRelated_Duration','Informational_Duration'],columns=['VisitorType'],aggfunc='mean').plot(kind='bar',figsize=(10,5))

data_converted.hist(bins=50,figsize=(10,14))

Month={'Feb':2, 'Mar':3, 'May':5, 'Oct':10, 'June':6, 'Jul':7, 'Aug':8, 'Nov':11, 'Sep':9,'Dec':12}
data_converted['Month']=data_converted['Month'].map(Month)

VisitorType={'Returning_Visitor':3, 'New_Visitor':2, 'Other':1}
data_converted['VisitorType']=data_converted['VisitorType'].map(VisitorType)

d={True:1,False:0}
data_converted['Weekend']=data_converted['Weekend'].map(d)
data_converted['Revenue']=data_converted['Revenue'].map(d)
plt.figure(figsize=(10,20))
sns.heatmap(data_converted.corr(),annot=True,cmap='viridis',linewidths=.5)

from sklearn.preprocessing import StandardScaler
# scaled or normalized, and the column names are preserved from the original
# DataFrame (data) except for the last column.
SS=StandardScaler()
scaled_features=SS.fit_transform(data.drop('Revenue',axis=1))
scaled_features

data_feat=pd.DataFrame(scaled_features,columns=data.columns[:-1])
data_feat.head()

#We need split train data and testdata. we can apply that for any one of the algo once we split

X_train, X_test, y_train, y_test = train_test_split(scaled_features, data['Revenue'], test_size=0.33, random_state=42)

knn=KNeighborsClassifier(n_neighbors=17)

knn.fit(X_train,y_train)

pred=knn.predict(X_test)

#now we are ready to take the metrics for our KNN classifier
from sklearn.metrics import confusion_matrix,classification_report
print(confusion_matrix(y_test,pred))

# to make the report our confustion metrics using skplot
!pip install -q scikit-plot
import scikitplot  as skplt

skplt.metrics.plot_confusion_matrix(y_test,pred,normalize=True)
print(classification_report(y_test,pred))

# Question 1: How do different informative page categories contribute to the likelihood of a user making a purchase?

# Display basic statistics of numerical features
print(data.describe())

# Visualize the distribution of the target variable 'Revenue'
import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(x='Revenue', data=data)
plt.show()

# Select relevant features
selected_features = data[['Administrative', 'Informational', 'ProductRelated',
                          'BounceRates', 'ExitRates', 'PageValues',
                          'SpecialDay', 'Revenue']]

# Machine Learning Classification:
# Build a machine learning classification model to predict the likelihood of a purchase.
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Split the data into features (X) and target variable (y)
X = data.drop('Revenue', axis=1)
y = data['Revenue']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Build and train a Logistic Regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))


# Feature Importance Analysis:
# Assess feature importance to identify which informative page categories contribute significantly.

# Extract feature importance from the model
feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': model.coef_[0]})
feature_importance = feature_importance.sort_values(by='Importance', ascending=False)

# Visualize feature importance
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance)
plt.title('Feature Importance')
plt.show()


Example Code Adjustments: For Additional EDA:
# Visualize the distribution of time spent on Informational and Product Related pages
sns.boxplot(x='Revenue', y='Informational_Duration', data=data)
plt.title('Distribution of Informational Page Duration by Revenue')
plt.show()

# Conduct a statistical test to compare the time spent on Informational pages between sessions with and without revenue
from scipy.stats import ttest_ind
result = ttest_ind(data[data['Revenue'] == 1]['Informational_Duration'], data[data['Revenue'] == 0]['Informational_Duration'])
print("T-Test Result for Informational Page Duration:", result)

# For Feature Engineering:
# Create aggregated features for total time spent on Informational and Product Related pages
data['Total_Informational_Duration'] = data['Informational_Duration'] * data['Informational']
data['Total_ProductRelated_Duration'] = data['ProductRelated_Duration'] * data['ProductRelated']

# Custom visualization showing the impact of Informational and Product Related pages on purchase likelihood
plt.figure(figsize=(12, 6))
sns.scatterplot(x='Total_Informational_Duration', y='Total_ProductRelated_Duration', hue='Revenue', data=data)
plt.title('Impact of Informational and Product Related Pages on Purchase Likelihood')
plt.xlabel('Total Time Spent on Informational Pages')
plt.ylabel('Total Time Spent on Product Related Pages')
plt.show()

# Regression Model
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

# Assuming 'PageValues' is the continuous variable you want to predict

# Split the data into features (X) and target variable (y)
X_reg = data.drop('PageValues', axis=1)
y_reg = data['PageValues']

# Split the data into training and testing sets
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)

# Build and train a Linear Regression model
lr_model = LinearRegression()
lr_model.fit(X_train_reg, y_train_reg)

# Make predictions
y_pred_reg = lr_model.predict(X_test_reg)

# Evaluate the Linear Regression model
print("Mean Squared Error:", mean_squared_error(y_test_reg, y_pred_reg))
print("R-squared:", r2_score(y_test_reg, y_pred_reg))


# Decision Tree
from sklearn.tree import DecisionTreeClassifier

# Build and train a Decision Tree model
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)

# Make predictions
dt_y_pred = dt_model.predict(X_test)

# Evaluate the Decision Tree model
print("Decision Tree Accuracy:", accuracy_score(y_test, dt_y_pred))
print("Decision Tree Classification Report:\n", classification_report(y_test, dt_y_pred))


# Random Forests:
from sklearn.ensemble import RandomForestClassifier

# Build and train a Random Forest model
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions
rf_y_pred = rf_model.predict(X_test)

# Evaluate the Random Forest model
print("Random Forest Accuracy:", accuracy_score(y_test, rf_y_pred))
print("Random Forest Classification Report:\n", classification_report(y_test, rf_y_pred))


# Support Vector Machines (SVM):
from sklearn.svm import SVC

# Build and train a Support Vector Machine model
svm_model = SVC(random_state=42)
svm_model.fit(X_train, y_train)

# Make predictions
svm_y_pred = svm_model.predict(X_test)

# Evaluate the Support Vector Machine model
print("SVM Accuracy:", accuracy_score(y_test, svm_y_pred))
print("SVM Classification Report:\n", classification_report(y_test, svm_y_pred))


# cross-validation and evaluate the performance metrics
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
import numpy as np

# Assuming 'PageValues' is the continuous variable you want to predict
X_reg = data.drop('PageValues', axis=1)
y_reg = data['PageValues']

# Linear Regression
lr_model = LinearRegression()
lr_scores = cross_val_score(lr_model, X_reg, y_reg, cv=5, scoring='neg_mean_squared_error')
lr_rmse_scores = np.sqrt(-lr_scores)
print("Linear Regression RMSE:", lr_rmse_scores.mean())

# Decision Tree Regression
dt_model = DecisionTreeRegressor()
dt_scores = cross_val_score(dt_model, X_reg, y_reg, cv=5, scoring='neg_mean_squared_error')
dt_rmse_scores = np.sqrt(-dt_scores)
print("Decision Tree Regression RMSE:", dt_rmse_scores.mean())

# Random Forest Regression
rf_model = RandomForestRegressor()
rf_scores = cross_val_score(rf_model, X_reg, y_reg, cv=5, scoring='neg_mean_squared_error')
rf_rmse_scores = np.sqrt(-rf_scores)
print("Random Forest Regression RMSE:", rf_rmse_scores.mean())

# Support Vector Regression
svr_model = SVR()
svr_scores = cross_val_score(svr_model, X_reg, y_reg, cv=5, scoring='neg_mean_squared_error')
svr_rmse_scores = np.sqrt(-svr_scores)
print("Support Vector Regression RMSE:", svr_rmse_scores.mean())


##Feature Selection Techniques: for question 1
# Feature selection is a valuable approach to identify the most important features that contribute 
# significantly to the prediction or outcome.

# Correlation Analysis:
#Calculate the correlation between each feature and the target variable ('Revenue').
#Select features with high correlation values.
# Assuming 'data' is the DataFrame

correlation_matrix = data.corr()
correlation_with_target = correlation_matrix['Revenue'].sort_values(ascending=False)
relevant_features_corr = correlation_with_target[abs(correlation_with_target) > 0.1].index.tolist()

# Display the relevant features
print("Correlation Analysis Relevant Features:")
print(relevant_features_corr)

# Recursive Feature Elimination (RFE):
# Use RFE with a machine learning classifier to recursively remove the least important features.
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier

# Assuming 'X' is your feature matrix and 'y' is your target variable
model = RandomForestClassifier()
rfe = RFE(model, n_features_to_select=1)
fit = rfe.fit(X, y)

relevant_features_rfe = X.columns[fit.ranking_ == 1].tolist()

#Tree-based Methods (e.g., Decision Trees, Random Forest):
# Train a tree-based model and evaluate feature importance.

from sklearn.ensemble import RandomForestClassifier

# Assuming 'X' is your feature matrix and 'y' is your target variable
model = RandomForestClassifier()
model.fit(X, y)

feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': model.feature_importances_})
relevant_features_tree = feature_importance.sort_values(by='Importance', ascending=False)['Feature'].tolist()


## Research Question 2: Can we predict the likelihood of a user making a purchase based on metrics such as
Bounce Rates, Exit Rates, and Page Values?

# Select relevant features
selected_features = data[['BounceRates', 'ExitRates', 'PageValues', 'Revenue']]

# Encode categorical variables if any

# Split the data into features (X) and target variable (y)
X = data[['BounceRates', 'ExitRates', 'PageValues']]
y = data['Revenue']

# Model Building and Evaluation:

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Build and train Logistic Regression model
logistic_model = LogisticRegression()
logistic_model.fit(X_train, y_train)

# Build and train Decision Tree model
tree_model = DecisionTreeClassifier()
tree_model.fit(X_train, y_train)

# Build and train Random Forest model
forest_model = RandomForestClassifier()
forest_model.fit(X_train, y_train)

# Make predictions
logistic_pred = logistic_model.predict(X_test)
tree_pred = tree_model.predict(X_test)
forest_pred = forest_model.predict(X_test)

# Evaluate models
print("Logistic Regression Accuracy:", accuracy_score(y_test, logistic_pred))
print("Decision Tree Accuracy:", accuracy_score(y_test, tree_pred))
print("Random Forest Accuracy:", accuracy_score(y_test, forest_pred))

# Classification reports, confusion matrices, and other metrics can also be explored.



# Feature Importance Analysis:

# Examine feature importance for Decision Tree or Random Forest models
feature_importance_tree = pd.DataFrame({'Feature': X.columns, 'Importance': tree_model.feature_importances_})
feature_importance_tree = feature_importance_tree.sort_values(by='Importance', ascending=False)
print("Decision Tree Feature Importance:\n", feature_importance_tree)

feature_importance_forest = pd.DataFrame({'Feature': X.columns, 'Importance': forest_model.feature_importances_})
feature_importance_forest = feature_importance_forest.sort_values(by='Importance', ascending=False)
print("Random Forest Feature Importance:\n", feature_importance_forest)

## analyze feature importance for Logistic Regression and compare it with Decision Trees and Random 
# Logistic Regression Coefficients
logistic_coefficients = pd.DataFrame({'Feature': X.columns, 'Coefficient': logistic_model.coef_[0]})
logistic_coefficients = logistic_coefficients.sort_values(by='Coefficient', ascending=False)
print("Logistic Regression Coefficients:\n", logistic_coefficients)

# Decision Tree Feature Importance
feature_importance_tree = pd.DataFrame({'Feature': X.columns, 'Importance': tree_model.feature_importances_})
feature_importance_tree = feature_importance_tree.sort_values(by='Importance', ascending=False)
print("Decision Tree Feature Importance:\n", feature_importance_tree)

# Random Forest Feature Importance
feature_importance_forest = pd.DataFrame({'Feature': X.columns, 'Importance': forest_model.feature_importances_})
feature_importance_forest = feature_importance_forest.sort_values(by='Importance', ascending=False)
print("Random Forest Feature Importance:\n", feature_importance_forest)


## Beside classification other analytics
#Correlation Analysis:

# Assuming 'data' is your DataFrame
correlation_matrix = data[['BounceRates', 'ExitRates', 'PageValues', 'Revenue']].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

#Distribution Plots:
# Example for Bounce Rates
plt.figure(figsize=(10, 6))
sns.kdeplot(data[data['Revenue'] == 1]['BounceRates'], label='Purchase', shade=True)
sns.kdeplot(data[data['Revenue'] == 0]['BounceRates'], label='No Purchase', shade=True)
plt.title('Distribution of Bounce Rates by Revenue')
plt.show()

#Anomaly Detection:
from sklearn.ensemble import IsolationForest

# Example for Bounce Rates anomaly detection
isolation_forest = IsolationForest(contamination=0.05)
data['BounceRates_Anomaly'] = isolation_forest.fit_predict(data[['BounceRates']])

# Statistical Tests:
from scipy.stats import ttest_ind

# Example for Bounce Rates
purchased_bounce_rates = data[data['Revenue'] == 1]['BounceRates']
not_purchased_bounce_rates = data[data['Revenue'] == 0]['BounceRates']

t_stat, p_value = ttest_ind(purchased_bounce_rates, not_purchased_bounce_rates)
print(f'Test Statistic: {t_stat}, p-value: {p_value}')

# Customer Segmentation:
from sklearn.cluster import KMeans

# Assuming 'data' is your DataFrame
features_for_clustering = data[['BounceRates', 'ExitRates', 'PageValues']]

# Scale the features if needed
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
features_for_clustering_scaled = scaler.fit_transform(features_for_clustering)

# Apply KMeans clustering
kmeans = KMeans(n_clusters=3, random_state=42)
data['Cluster'] = kmeans.fit_predict(features_for_clustering_scaled)

